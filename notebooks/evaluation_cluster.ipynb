{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# DL Stack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main.models.classification_layers import ClassifierLayer\n",
    "from main.utils.loaders import FaceDataSetPrep\n",
    "from main.fairness.quantile_transport import Calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need:\n",
    "* Split ids\n",
    "* Data Loader\n",
    "* Model Arch\n",
    "* Model weights\n",
    "* Calibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from main.utils.loaders import FaceDataSetPrep\n",
    "from main.fairness.quantile_transport import Calibrator\n",
    "\n",
    "def calculate_metrics(model, \n",
    "                      split_location, \n",
    "                      prediction_feature,\n",
    "                      sensitive_feature,\n",
    "                      awareness,\n",
    "                      data_location='./data/training/label_loader.csv', \n",
    "                      batch_size=256,\n",
    "                      device='cpu'):\n",
    "    \n",
    "    # Set up device, load data\n",
    "    device_ = device\n",
    "    return_dict = {}\n",
    "\n",
    "    with open(split_location, 'rb') as con_:\n",
    "        splits_dict = pickle.load(con_)\n",
    "\n",
    "    data_labels = pd.read_csv(data_location)\n",
    "\n",
    "    data_calibration = (data_labels\n",
    "                        .loc[data_labels.img_id\n",
    "                             .isin(splits_dict['calib'])])\n",
    "    \n",
    "    data_test = (data_labels\n",
    "                 .loc[data_labels.img_id\n",
    "                      .isin(splits_dict['calib'])])\n",
    "    \n",
    "    idx_sensitive = int(np.where(data_labels.columns == sensitive_feature)[0])\n",
    "    idx_prediction = int(np.where(data_labels.columns == prediction_feature)[0])\n",
    "\n",
    "\n",
    "    # Prepare dataloaders\n",
    "    data_set_calib = FaceDataSetPrep(data_calibration, \n",
    "                                     label_idx=idx_prediction, \n",
    "                                     sens_idx=idx_sensitive)\n",
    "\n",
    "    data_set_test = FaceDataSetPrep(data_test, \n",
    "                                    label_idx=idx_prediction, \n",
    "                                    sens_idx=idx_sensitive)\n",
    "\n",
    "    calib_loader = DataLoader(data_set_calib, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False)\n",
    "\n",
    "    test_loader = DataLoader(data_set_test, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False)\n",
    "    \n",
    "    # Run predictions on calibration set\n",
    "    calib_predictions = []\n",
    "    calib_labels = []\n",
    "    calib_sens = []\n",
    "\n",
    "    for i, (inputs, labels, input_fair) in enumerate(calib_loader):\n",
    "        inputs, labels, inputs_fair = inputs.to(device_), labels.to(device_), input_fair.to(device_)\n",
    "\n",
    "        if awareness:    \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.squeeze(1), inputs_fair)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.squeeze(1))\n",
    "\n",
    "        if device_ != 'cpu':\n",
    "            out_ = torch.sigmoid(outputs.squeeze()).cpu().detach().numpy()\n",
    "            labs_ = labels.cpu().detach().numpy()\n",
    "            labs_sensitive = inputs_fair.cpu().detach().numpy()\n",
    "        else:\n",
    "            out_ = torch.sigmoid(outputs.squeeze()).detach().numpy()\n",
    "            labs_ = labels.detach().numpy()\n",
    "            labs_sensitive = inputs_fair.detach().numpy()\n",
    "\n",
    "        calib_predictions.append(out_)\n",
    "        calib_labels.append(labs_)\n",
    "        calib_sens.append(labs_sensitive)\n",
    "\n",
    "    preds_calib = np.concatenate(calib_predictions)\n",
    "    labs_calib = np.concatenate(calib_labels)\n",
    "    sensitive_calib = np.concatenate(calib_sens)\n",
    "\n",
    "    return_dict['calib'] = {}\n",
    "    return_dict['calib']['preds'] = preds_calib\n",
    "    return_dict['calib']['labs'] = labs_calib\n",
    "    return_dict['calib']['sensitive'] = sensitive_calib\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(labs_calib,\n",
    "                                    preds_calib,\n",
    "                                    pos_label=1)\n",
    "\n",
    "    auc_calib = metrics.auc(fpr, tpr)\n",
    "    print(f'AUC Calibration: {auc_calib}')\n",
    "\n",
    "    return_dict['metrics'] = {}\n",
    "    return_dict['metrics']['auc_calibration'] = auc_calib\n",
    "\n",
    "    # Fit Calibrator\n",
    "    calibrator_ = Calibrator()\n",
    "    calibrator_.fit(preds_calib,\n",
    "                    sensitive_calib)\n",
    "    \n",
    "    ## Run Test predictions\n",
    "    test_preds = []\n",
    "    test_labs = []\n",
    "    test_sensitive = []\n",
    "\n",
    "    for i, (inputs, labels, input_fair) in enumerate(test_loader):\n",
    "        inputs, labels, inputs_fair = inputs.to(device_), labels.to(device_), input_fair.to(device_)\n",
    "\n",
    "        if awareness:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.squeeze(1), inputs_fair)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.squeeze(1))\n",
    "\n",
    "        if device_ != 'cpu':\n",
    "            out_ = torch.sigmoid(outputs.squeeze()).cpu().detach().numpy()\n",
    "            labs_ = labels.cpu().detach().numpy()\n",
    "            labs_sensitive = inputs_fair.cpu().detach().numpy()\n",
    "        else:\n",
    "            out_ = torch.sigmoid(outputs.squeeze()).detach().numpy()\n",
    "            labs_ = labels.detach().numpy()\n",
    "            labs_sensitive = inputs_fair.detach().numpy()\n",
    "\n",
    "        test_preds.append(out_)\n",
    "        test_labs.append(labs_)\n",
    "        test_sensitive.append(labs_sensitive)\n",
    "\n",
    "    preds_test = np.concatenate(test_preds)\n",
    "    labs_test = np.concatenate(test_labs)\n",
    "    sensitive_test = np.concatenate(test_sensitive)\n",
    "\n",
    "    return_dict['test'] = {}\n",
    "    return_dict['test']['preds'] = preds_test\n",
    "    return_dict['test']['labs'] = labs_test\n",
    "    return_dict['test']['sensitive'] = sensitive_test\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(labs_test,\n",
    "                                    preds_test,\n",
    "                                    pos_label=1)\n",
    "\n",
    "    auc_test_unfair = metrics.auc(fpr, tpr)\n",
    "    print(f'AUC-Test, unfair: {auc_test_unfair}')\n",
    "\n",
    "    return_dict['metrics']['auc_test_unfair'] = auc_test_unfair\n",
    "\n",
    "    scores_test_fair = calibrator_.transform(preds_test,\n",
    "                                             sensitive_test)\n",
    "    \n",
    "    return_dict['test']['preds_fair'] = scores_test_fair\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(labs_test,\n",
    "                                    scores_test_fair,\n",
    "                                    pos_label=1)\n",
    "\n",
    "    auc_test_fair = metrics.auc(fpr, tpr)\n",
    "    print(f'AUC-Test, fair: {auc_test_fair}')\n",
    "    return_dict['metrics']['auc_test_fair'] = auc_test_fair\n",
    "\n",
    "    # Calculate unfairness\n",
    "    unfairness_test = stats.ks_2samp(preds_test[sensitive_test==1],\n",
    "                                     preds_test[sensitive_test==0],\n",
    "                                     alternative='two-sided')[0]\n",
    "    unfairness_corrected_test = stats.ks_2samp(scores_test_fair[sensitive_test==1],\n",
    "                                               scores_test_fair[sensitive_test==0],\n",
    "                                               alternative='two-sided')[0]\n",
    "    \n",
    "    return_dict['metrics']['fairness_uncorrected'] = unfairness_test\n",
    "    return_dict['metrics']['fairness_corrected'] = unfairness_corrected_test\n",
    "\n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_string = './results_cluster/splits_seed_42.pkl'\n",
    "model_pred_ = ClassifierLayer([512,256,64])\n",
    "model_pred_.load_state_dict(torch.load('./data/results/models/aware/models/model_simple_42_beard.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Calibration: 0.9406463730030297\n",
      "AUC-Test, unfair: 0.9408697138344602\n",
      "AUC-Test, fair: 0.7653758492799391\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = calculate_metrics(model=model_pred_, \n",
    "                                 split_location=split_string, \n",
    "                                 prediction_feature='No_Beard', \n",
    "                                 sensitive_feature='Male', \n",
    "                                 awareness=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_calibration': 0.8574212915358411,\n",
       " 'auc_test_unfair': 0.8587327781750448,\n",
       " 'auc_test_fair': 0.81096720518655,\n",
       " 'fairness_uncorrected': 0.31564992330775826,\n",
       " 'fairness_corrected': 0.005564680794877486}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on all results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Calibration: 0.9390738251599217\n",
      "AUC-Test, unfair: 0.9396422233470364\n",
      "AUC-Test, fair: 0.7404997008893719\n",
      "AUC Calibration: 0.9406073453648611\n",
      "AUC-Test, unfair: 0.9403533816810408\n",
      "AUC-Test, fair: 0.7596128883970965\n",
      "AUC Calibration: 0.9412984893914752\n",
      "AUC-Test, unfair: 0.9409192880342978\n",
      "AUC-Test, fair: 0.7730653371270315\n",
      "AUC Calibration: 0.944030299456841\n",
      "AUC-Test, unfair: 0.9440343911590161\n",
      "AUC-Test, fair: 0.767317047581947\n",
      "AUC Calibration: 0.9411418497725421\n",
      "AUC-Test, unfair: 0.941767944127714\n",
      "AUC-Test, fair: 0.7732084834827461\n",
      "AUC Calibration: 0.9404055173299612\n",
      "AUC-Test, unfair: 0.9399710641355836\n",
      "AUC-Test, fair: 0.7648107168719924\n",
      "AUC Calibration: 0.9380435837430515\n",
      "AUC-Test, unfair: 0.9373770997423702\n",
      "AUC-Test, fair: 0.7756606011464631\n",
      "AUC Calibration: 0.9451088679631046\n",
      "AUC-Test, unfair: 0.9450098720743406\n",
      "AUC-Test, fair: 0.8070062590630411\n",
      "AUC Calibration: 0.9414216502721509\n",
      "AUC-Test, unfair: 0.9420197033902054\n",
      "AUC-Test, fair: 0.7541828328051344\n",
      "AUC Calibration: 0.9441718581991684\n",
      "AUC-Test, unfair: 0.9428980871265877\n",
      "AUC-Test, fair: 0.7930050668804742\n"
     ]
    }
   ],
   "source": [
    "auc_unfair_list = []\n",
    "auc_fair_list = []\n",
    "unfairness_unfair = []\n",
    "unfairness_fair = []\n",
    "\n",
    "want_string = 'beard'\n",
    "\n",
    "for mod_string in os.listdir('./data/results/models/aware/models/'):\n",
    "    if want_string in mod_string:\n",
    "        seed_string = re.findall('\\d+', mod_string)[0]\n",
    "\n",
    "        # Load up \n",
    "        split_string = f'./data/results/split_idx/splits_seed_{seed_string}.pkl'\n",
    "        model_pred_ = ClassifierLayer([512,256,64])\n",
    "        model_pred_.load_state_dict(torch.load(f'./data/results/models/aware/models/model_simple_{seed_string}_{want_string}.pt'))\n",
    "        model_pred_.train()\n",
    "\n",
    "        metrics_dict = calculate_metrics(model=model_pred_, \n",
    "                                        split_location=split_string, \n",
    "                                        prediction_feature='No_Beard', \n",
    "                                        sensitive_feature='Male', \n",
    "                                        awareness=True)\n",
    "        \n",
    "        auc_fair_list.append(metrics_dict['metrics']['auc_test_fair'])\n",
    "        auc_unfair_list.append(metrics_dict['metrics']['auc_test_unfair'])\n",
    "        unfairness_unfair.append(metrics_dict['metrics']['fairness_uncorrected'])\n",
    "        unfairness_fair.append(metrics_dict['metrics']['fairness_corrected'])\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.941 & 0.896 & 0.771 & 0.531 \\\\\n",
      "& $\\pm$0.002 & $\\pm$ 0.01 & $\\pm$ 0.018 & $\\pm$ 0.096 \\\\\n"
     ]
    }
   ],
   "source": [
    "fair_fair = np.round(np.array(unfairness_fair).mean(),3)\n",
    "fair_fair_std = np.round(np.array(unfairness_fair).std(),3)\n",
    "\n",
    "fair_unfair = np.round(np.array(unfairness_unfair).mean(),3)\n",
    "fair_unfair_std = np.round(np.array(unfairness_unfair).std(),3)\n",
    "\n",
    "auc_fair = np.round(np.array(auc_fair_list).mean(),3)\n",
    "auc_fair_std = np.round(np.array(auc_fair_list).std(),3)\n",
    "\n",
    "auc_unfair = np.round(np.array(auc_unfair_list).mean(),3)\n",
    "auc_unfair_std = np.round(np.array(auc_unfair_list).std(),3)\n",
    "\n",
    "print(f'& {auc_unfair} & {fair_unfair} & {auc_fair} & {fair_fair} \\\\\\\\')\n",
    "print(f'& $\\pm${auc_unfair_std} & $\\pm$ {fair_unfair_std} & $\\pm$ {auc_fair_std} & $\\pm$ {fair_fair_std} \\\\\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv_imgs",
   "language": "python",
   "name": "cenv_imgs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
